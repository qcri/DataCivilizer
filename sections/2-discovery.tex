% !TEX root = ../main.tex
\section{Discovery}
\label{sec:discovery}

The goal of the data discovery module is to find relevant data among multiple,
perhaps millions of, datasets that spread across the storage systems of an
enterprise.

A naive solution is to ask an expert (if one is available), or to perform
manual exploration by inspecting datasets one by one, which is obviously
time-consuming and prone to missing relevant data sources.  Recently, we are
witnessing efforts to automate this process, such as
Goods~\cite{DBLP:conf/sigmod/HalevyKNOPRW16} or
InfoGather~\cite{DBLP:conf/sigmod/YakoutGCC12}. Unlike these systems
that are designed to answer a particular use case, our discovery module aims to
collect enough information about the datasets and to expose them to upper layers
of \dcv allowing the orchestration of query execution, which is the ultimate
goal.

The data discovery module narrows down the search for relevant data from
$n$ data sources to a handful number of $m$ data sources, upon which we can afford to compute finer-grained relationships.
\dcv discovery supports a user-level API to search for relevant data using a
variety of techniques such as schema search and similar content. 
%Discovery is extensible; an engineer can extend the supported relationships to cover other relevant use cases.

\subsection{Data Profiler}

We summarize each column of each table into a {\em profile}. Each profile contains a signature, which is a domain-dependent,
compact representation of the original content.  For numerical data, we use the
data distribution, and for textual data,  a vector of the most significant words
in the column.  Our profiles also contain information about data cardinality,
data type, and numerical ranges if applicable~\cite{profiling_survey}.

Note that finding pairwise relationships between each attribute profile is an $O(n^2)$ computation, which will not scale.  
We thus rely on locality sensitive hashing
(LSH)~\cite{DBLP:conf/compgeom/DatarIIM04} to address this issue. Specifically,
we hash each signature and group together those that collide.  These are more
likely to be similar; we only need to perform comparisons among the nodes in
each cluster, ignoring cross cluster comparisons.  This approach avoids $n^2$
operations and appears to perform well in practice.

%\subsection{Data Discovery Components}
%
%The data discovery module consists of two conceptually different components, a
%data profiler and a graph builder, that collaborate to build the linkage graph
%indicated in Figure~\ref{fig:arch}. 
%
%\begin{enumerate}
%\item {\bf Data profiler.} This component summarizes each column of each table
%into a profile. Each profile contains a signature, which is a domain-dependent,
%compact representation of the original content.  For numerical data, we use the
%data distribution, and for textual data,  a vector of the most significant words
%in the column.  Our profiles also contain information about data cardinality,
%data type, and numerical ranges if applicable~\cite{profiling_survey}.
%
%\item {\bf Graph builder.} Using the profiles, the graph builder finds
%relationships among nodes and represents them as edges. Examples of
%relationships are \emph{content similarity} based on measuring the similarity
%distance between signatures and \emph{schema similarity} that captures the
%similarity of the labels of the different nodes. Another kind of relationship
%captures the hierarchical relationship between a table and its columns.
%\nan{Each node is a column, or a table? The graph is not formally defined.}
%\sibo{The definition of node appears in Section \ref{introduction}, but it will
%be better to recap the definition again.}
%
%\end{enumerate}
%
%The graph resulting from this process can be used to answer discovery queries.
%Discovery queries vary from simple keyword search, useful to quickly glance over
%data, to more involved queries such as table augmentation, to add attributes of
%interest to known tables. We discuss the queries at the end of the section. 

\subsection{Data Profiling at Scale}

To run at scale, discovery relies on sampling to estimate the cardinality of
columns and the quantiles for numerical data. This avoids having to sort each
column, which is not a linear computation.  More generally, this avoids a
requirement to copy or update the data, reducing memory consumption.

The data profiler architecture is a pipeline of stages, with each stage
responsible for performing a different computation. For example, the first stage
performs basic denoising of the data, such as removing empty values and dealing
with potential formatting issues of the underlying files, e.g. figuring out the
separator for a CSV file.  The second stage determines the data type,
information that is propagated along with the data to the next stages. The rest
of the stages are responsible for computing cardinalities, performing quantile
estimation, e.g., to determine numerical ranges.

The profiler has connectors to read data from different data sources, such as
HDFS files, RDBMS or CSV files. The data is then streamed through the profiler
pipeline which outputs the results
%and  at the end the resulting attribute profile is sent
to a store from where it is later consumed by the graph builder.
%At the same time,
The profiler works in a distributed environment using a lightweight coordinator
that splits the work among multiple processors, helping to scale the
computation.

%The discovery module benefits from our distributed architecture that allows us to parallelize profiling as well as graph building, further speeding up execution times. Other kinds of relationships such as finding FK-PKs are too expensive to compute at scale for the whole graph.  These are left to the stitching module, which is described next.


\subsection{Discovery Queries}
\label{subsec:api}

The discovery system permits users to submit \emph{schema retrieval queries},
that are used to find schemas, i.e., datasets, relevant to the task at hand. For
example, a user may use discovery to find all datasets that contain names of
employees in a company with a schema retrieval query that search for all
attributes referring to employee or name, and then write a SQL query (data
retrieval query) to filter from a list. Next we give some examples of some functions
currently available to users:

\begin{itemize}
\item \textbf{Fill-in schema}. Given a set of names, the discovery system
retrieves and shows tables or groups of (probably joinable) tables that contain
the desired attributes. The core of the operation consists of finding tables
with similar attribute names to the provided ones.
\item \textbf{Extend attribute.} Similar to the extend operator of Octopus
\cite{octopus}, or the ABA operation of Infogather \cite{DBLP:conf/sigmod/YakoutGCC12}, discovery
can extend tables of interest with additional attributes requested by users. The
core of the operation consists of finding matches to the current table, and then
retrieving the attributes that do not appear in the original table.
\item \textbf{Subsumption relationships.} Given some table of reference,
discovery can provide a list of tables or groups of tables that have some form
of subsumption relationship with respect to the reference one. 
\item \textbf{Similarity and lookup.} Discovery can also be used to find schemas
that contain data similar to some provided schema (attribute or table), as well
as to find schemas that contain certain values, i.e., kewyord search for
textual data and range search for numerical data.
\end{itemize}

Discovery works as an information retrieval system: a schema retrieval query
does not have a specific answer, instead it returns a ranked set of results.
Hence, ranking appropiately the results from the hundreds of underlying data
sources falls in the domain of discovery, and it is of great importance to reduce
the amount of work that downstream components to discovery within Data Civilizer
must perform.

