% !TEX root = ../main.tex
\section{Discovery}
\label{sec:discovery}

The data discovery module's goal is to find relevant data from hundreds, thousands or
 millions of data sets  spread across the different storage systems of an
enterprise.

Typical solutions to finding relevant data include asking an expert (if one is available), or  performing
manual exploration by inspecting datasets one by one.  Obviously, both approaches are
time-consuming and prone to missing relevant data sources.  
Recently, there have been efforts to automate this process, through efforts such as
Goods~\cite{DBLP:conf/sigmod/HalevyKNOPRW16} and 
InfoGather~\cite{DBLP:conf/sigmod/YakoutGCC12}. \srm{I don't understand the following sentence, vis-a-vis the Goods paper, which seems to be about supporting data discovery for a bunch of use cases, without one specific use-case in mind.}  Unlike these systems
that are designed with a specific use-case in mind, our discovery module aims to
collect enough information about the data sets and to expose them to upper layers
of \dcv, allowing the orchestration of complex query workflows that integrate a variety of cleaning, linking, and querying operations.
%execution, which is the ultimate goal.  

The data discovery module narrows down the search for relevant data from
$n$ data sources to a handful of $m$ data sources, on which finer-grained relationships can be computed more efficiently.
\dcv discovery supports a user-level API to search for relevant data using a
variety of techniques such as searches for tables with specific substrings in the schemas ({\it schema search}) or table values ({\it content search}). 
%Discovery is extensible; an engineer can extend the supported relationships to cover other relevant use cases.

\subsection{Data Profiling at Scale}

The key idea in the discovery module is to summarize each column of each table into a {\em profile}.
A profile consists of one or more {\it signatures};  each signature summarizes the original content of the column into a domain-dependent,
compact representation of the original data.  By default signatures consist of a histogram representing the
data distribution of numerical values and  a vector of the most significant words
in the column for textual data.  Our profiles also contain information about data cardinality,
data type, and numerical ranges if applicable~\cite{profiling_survey}.

%Note that finding pairwise relationships between each attribute profile is an $O(n^2)$ computation, which will not scale.  
%We thus rely on locality sensitive hashing
%(LSH)~\cite{DBLP:conf/compgeom/DatarIIM04} to address this issue. Specifically,
%we hash each signature and group together the colliding ones. %those that
%% collide.
%These are more likely to be similar; we only need to perform comparisons among the nodes in
%each cluster, ignoring cross cluster comparisons.  This approach avoids $O(n^2)$
%operations and appears to perform well in practice.

%\subsection{Data Discovery Components}
%
%The data discovery module consists of two conceptually different components, a
%data profiler and a graph builder, that collaborate to build the linkage graph
%indicated in Figure~\ref{fig:arch}. 
%
%\begin{enumerate}
%\item {\bf Data profiler.} This component summarizes each column of each table
%into a profile. Each profile contains a signature, which is a domain-dependent,
%compact representation of the original content.  For numerical data, we use the
%data distribution, and for textual data,  a vector of the most significant words
%in the column.  Our profiles also contain information about data cardinality,
%data type, and numerical ranges if applicable~\cite{profiling_survey}.
%
%\item {\bf Graph builder.} Using the profiles, the graph builder finds
%relationships among nodes and represents them as edges. Examples of
%relationships are \emph{content similarity} based on measuring the similarity
%distance between signatures and \emph{schema similarity} that captures the
%similarity of the labels of the different nodes. Another kind of relationship
%captures the hierarchical relationship between a table and its columns.
%\nan{Each node is a column, or a table? The graph is not formally defined.}
%\sibo{The definition of node appears in Section \ref{introduction}, but it will
%be better to recap the definition again.}
%
%\end{enumerate}
%
%The graph resulting from this process can be used to answer discovery queries.
%Discovery queries vary from simple keyword search, useful to quickly glance over
%data, to more involved queries such as table augmentation, to add attributes of
%interest to known tables. We discuss the queries at the end of the section. 

%\subsection{Data Profiling at Scale}

To run at scale, discovery relies on sampling to estimate the cardinality of
columns and the quantiles for numerical data. This avoids having to sort each
column, which is not a linear computation.  This also allows us to avoid making a copy of
the data,  reducing memory pressure.

The profiler itself consists a pipeline of stages, where each stage is
responsible for performing a different computation. For example, the first stage
performs basic denoising of the data, such as removing empty values and dealing
with potential formatting issues of the underlying files, e.g. figuring out the
separator for a CSV file.  The second stage determines the data type,
information that is propagated along with the data to the next stages. The rest
of the stages are responsible for computing cardinalities, performing quantile
estimation, e.g., to determine numerical ranges.

The profiler has connectors to read data from different data sources, such as
HDFS files, RDBMS or CSV files. The data is then streamed through the profiler
pipeline which outputs the results
%and  at the end the resulting attribute profile is sent
to a store from where it is later consumed by the graph builder, which will be
explained in Section \ref{subsec:graphbuild}.
%At the same time,
The profiler works in a distributed environment using a lightweight coordinator
that splits the work among multiple processors, helping to scale the
computation.

%The discovery module benefits from our distributed architecture that allows us to parallelize profiling as well as graph building, further speeding up execution times. Other kinds of relationships such as finding FK-PKs are too expensive to compute at scale for the whole graph.  These are left to the stitching module, which is described next.


\subsection{Discovery Queries}
\label{subsec:api}

Using the discovery system, users can submit \emph{schema retrieval queries}
to find schemas, i.e., data sets, relevant to the task at hand. 
For example, to find all data sets that contain names of
employees in a company, the user issues a schema retrieval query that searches for all
attributes referring to the employee or name.
The user can then write a SQL query, i.e., a \emph{data retrieval query}, to filter from a list. 
Here are some examples of functions that the discovery module provides to users:

\begin{itemize}
\item \textbf{Fill-in schema}. Given a set of names, this function
retrieves and shows tables or groups of (probably joinable\mourad{How do you know?}) tables that contain
the desired attributes. The core of the operation consists of finding tables
with similar attribute names to the provided ones.

\item \textbf{Extend attribute.} Similar to the extend operator of Octopus~\cite{octopus}, or the ABA operation of Infogather~\cite{DBLP:conf/sigmod/YakoutGCC12}, using this function users
can extend tables of interest with additional attributes. The
core of the operation consists of finding matches to the current table, and then
retrieving the attributes that do not appear in the original table.

\item \textbf{Subsumption relationships.} Given some reference tables,
this function can provide a list of tables or groups of tables that have some form
of subsumption relationship with respect to the reference one. 

\item \textbf{Similarity and lookup.} Discovery can also be used to find schemas
that contain data similar to some provided schema (attribute or table), as well
as to find schemas that contain certain values, i.e., keyword search for
textual data and range search for numerical data.
\end{itemize}

In summary, discovery works as an information retrieval system: a schema retrieval query
does not have a specific answer, instead it returns a ranked set of results.
Hence, ranking appropriately the results from the hundreds of underlying data
sources falls in the domain of discovery, and it is of great importance to reduce
the amount of work that downstream components  within \dcv must perform.

