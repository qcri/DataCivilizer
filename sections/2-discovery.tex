% !TEX root = ../main.tex
\section{Discovery}
\label{sec:discovery}

The goal of the data discovery module is to find relevant data among multiple, perhaps millions of, datasets that spread across the storage systems of an enterprise.

A typical solution is to ask an expert (if one is available), or to perform manual exploration by inspecting datasets one by one, which is obviously time-consuming and prone to missing relevant data sources.
Recently, we are witnessing efforts to automate this process, such as \texttt{Goods}~\cite{DBLP:conf/sigmod/HalevyKNOPRW16} or \texttt{InfoGather}~\cite{ref}\dong{to add reference here}. Unlike these systems that are designed to answer a particular use case, our discovery module aims to collect enough information about the datasets and to expose them to upper layers of \dcv allowing the orchestration of query execution, which is the ultimate goal.

The data discovery module narrows down the search for relevant data from millions of data sources to a handful, upon which stitching can be performed. \dcv discovery supports a user-level API to search for relevant data using a variety of techniques such as schema search and similar content. Discovery is extensible; an engineer can extend the supported relationships to cover other relevant use cases.

\subsection{Data Discovery Components}

The data discovery module consists of two conceptually different components, a data profiler and a graph builder, that collaborate to build the linkage graph indicated in Figure~\ref{fig:arch}.

\begin{enumerate}
\item {\bf Data profiler.} This component summarizes each column of each table into a profile. Each profile contains a signature, which is a domain-dependent, compact representation of the original content.
For numerical data, we use the data distribution, and for textual data,  a vector of the most significant words in the column.
Our profiles also contain information about data cardinality, data type, and numerical ranges if applicable~\cite{profiling_survey}.

\item {\bf Graph builder.} Using the profiles, the graph builder finds relationships among nodes and represents them as edges. Examples of relationships are \emph{content similarity} based on measuring the similarity distance between signatures and \emph{schema similarity} that captures the similarity of the labels of the different nodes. Another kind of relationship captures the hierarchical relationship between a table and its columns. \nan{Each node is a column, or a table? The graph is not formally defined.} \sibo{The definition of node appears in Section \ref{introduction}, but it will be better to recap the definition again.}

\end{enumerate}


\subsection{Data Profiling at Scale}

To run at scale, discovery relies on sampling to estimate the cardinality of columns and the quantiles for numerical data. This avoids having to sort each column, which is not a linear computation.  More generally, this avoids a requirement to copy or update the data, reducing memory consumption.

The data profiler architecture is a pipeline of stages, with each stage responsible for performing a different computation. For example, the first stage performs basic denoising of the data, such as removing empty values and dealing with potential formatting issues of the underlying files, e.g. figuring out the separator for a CSV file.
The second stage determines the data type, information that is propagated along with the data to the next stages. The rest of the stages are responsible for computing cardinalities, performing quantile estimation, e.g., to determine numerical ranges.

The profiler has connectors to read data from different data sources, such as HDFS files, RDBMS or CSV files. The data is then streamed through the profiler pipeline
which outputs the results
%and  at the end the resulting attribute profile is sent
to a store from where it is later consumed by the graph builder.
%At the same time,
The profiler works in a distributed environment using a lightweight coordinator that splits the work among multiple processors, helping to scale the computation.



\subsection{Graph Builder}



The input to the graph builder is a set of profiles that are attached to its nodes. The graph builder computes the edges among pairs of nodes. In particular, it finds content and schema similarity relationships. Each edge is scored with a weight that represents the strength of the relationship: the particular meaning depends on the edge semantics.\mourad{Give examples.}  Also, edges with a similarity less than a user-defined threshold are discarded.
This avoids having many edges that are not significant.


Finding edges in the graph is an $O(n^2)$ computation, which will not scale.  We thus rely on locality sensitive hashing (LSH)~\cite{DBLP:conf/compgeom/DatarIIM04} to address this issue. Specifically, we hash each signature and group together those that collide.  These are more likely to be similar; we only need to perform comparisons among the nodes in each cluster, ignoring cross cluster comparisons.  This approach avoids $n^2$ operations and appears to perform well in practice.


The discovery module benefits from our distributed architecture that allows us to parallelize profiling as well as graph building, further speeding up execution times. Other kinds of relationships such as finding FK-PKs are too expensive to compute at scale for the whole graph.  These are left to the stitching module, which is described next.


