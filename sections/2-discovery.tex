% !TEX root = ../main.tex
\section{Discovery}
\label{sec:discovery}

The goal of the data discovery module is to find relevant data among the perhaps millions of datasets spread across the storage systems of an enterprise. 

A typical solution is to ask an expert (if one is available), or to perform manual exploration, inspecting datasets one by one (which is obviously time-consuming and prone to missing relevant data sources). Recently, we are witnessing efforts to automate this process, such as \texttt{Goods}~\cite{DBLP:conf/sigmod/HalevyKNOPRW16} or \texttt{InfoGather}~\cite{ref}\dong{to add reference here}. Unlike these systems that are designed to answer a particular use case, discovery aims to collect enough information about the datasets and expose them to upper layers of \dcv require to orchestrate query execution, which is the ultimate goal.

The data discovery module narrows down the search for relevant data from millions of data sources to a handful of them, upon which stitching can be performed. Data civilizer discovery supports a user-level API to search for relevant data using a variety of techniques such as schema search, similar content. Discovery is extensible: an engineer can extend the supported relationships to cover other relevant use cases.

\subsection{Data Discovery Components}

The data discovery module consists of two conceptually different components, a data profiler and a graph builder, that collaborate with each other to build the linkage graph indicated in Figure~\ref{fig:arch}.

\begin{enumerate}
\item {\bf Data profiler.} We summarize each column of each table into a profile. Each profile contains a signature, which is a domain-dependent, compact representation of the original content. Our signature for numerical data is a data distribution, and for textual data it is a vector of the most significant words in the column. Our profiles also contains information about data cardinality, data type, and numerical ranges if applicable~\cite{profiling_survey}.  

\item {\bf Graph builder.} Using profiles, the graph builder finds relationships among nodes, and represents them as edges. Examples of relationships are \emph{content similarity} based on measuring the similarity distance between signatures and \emph{schema similarity} that captures the similarity of the names of the different nodes. Another kind of relationship capture the hierarchical relationship between a table and its columns.

\end{enumerate}


\subsection{Data Profiles at Scale}

To run at scale, discovery relies on sampling to estimate the cardinality of columns and the quantiles for numerical data. This avoids having to sort each column, which is not a linear computation.  More generally, this avoids a requirement to copy or update the data, reducing memory consumption. 

The data profiler architecture is a pipeline of stages, with each stage responsible for performing a different computation. For example, the first stage performs basic denoising of the data, such as removing empty values and dealing with potential formatting issues of the underlying files, e.g. figuring out the separator for a CSV file. The second stage determines the data type, information that is propagated along with the data to the next stages. The rest of the stages are responsible for computing cardinalities, performing quantile estimation, i.e. to determine numerical ranges, etc. 

The profiler has connectors to read data from different data sources, such as HDFS files, RDBMS or CSV files. The data is then streamed through the profiler pipeline and at the end the resulting attribute profile is sent to a store from where it is later consumed by the graph builder. At the same time, the profiler works in a distributed environment, with a lightweight coordinator that splits the work among multiple processors, helping to scale the computation.



\subsection{Graph Builder} 



The input to the graph builder is a set of profiles, V, that are the attached to nodes of the graph. The graph builder then computes the arcs among pairs of nodes. In particular, the graph builder component finds content and schema similarity relationships. Each edge is scored with a weight that represents the strength of the relationship: the particular meaning depends on the edge semantics.  Also, edges with a similarity less than a user-defined threshold are discarded, to avoid having a graph with many edges that are not significant. 


Finding edges in the graph is an $O(n^2)$ computation, which will not scale.  Hence, we rely on locality sensitive hashing (LSH)~\cite{DBLP:conf/compgeom/DatarIIM04}. Specifically, we hash each signature and group together those that collide.  These are more likely to be similar, so we perform a comparison among the nodes in each cluster, ignoring cross cluster comparisons.  This approach avoids $n^2$ operations and appears to perform well in practice.


The discovery module benefits from our distributed architecture that allows us to parallelize profiling as well as graph building, further speeding up execution times. Other kind of relationships such as finding FK-PKs are too expensive to compute at scale for the whole graph.  These are left to the stitching module, which is described next. 


