% !TEX root = ../main.tex
\section{Discovery}
\label{sec:discovery}

The goal of the data discovery module is to find relevant data among the
millions of datasets spread across many storage systems of modern organizations.
Suppose an analyst wants to answer the question: what is the monthly sales trend
by department? The analyst knows conceptually what data is needed to answer this
question (a table of sales, a table of departments, a table of products sold by
each department), but not which specific data sources (which relations in a
schema or what files in an HDFS deployment) contain such data. The typical
solution is to 1) ask an expert (if such a person is available), or to perform
manual exploration, inspecting datasets one by one (which is time-consuming and
prone to missing relevant data sources). We say this analyst is facing a data
discovery challenge.

The data discovery module narrows down the search of relevant data from the
thousands and millions of data sources to a handful of them that can then be fed
to stitching---that performs the final preparation before processing. Discovery
exports an API that can be used by users directly to search for relevant data,
e.g., schema search, similar content, etc, as well as by other modules in Data
Civilizer, such as stitching, to retrieve additional data that can be used, for
example, to enrich a table. Next we describe the general steps of data
discovery:

\subsection{Data Discovery Components}

The data discovery module consists of two conceptually different components
that collaborate with each other, as shown in Figure~\ref{fig:arch}:

\begin{enumerate}
\item {\bf Data profiler.} We first accumulate knowledge of the data sources by
summarizing them into concise profiles. A profile contains a signature, which is
a domain-dependent, compact representation of the original content. It also
contains information about the data cardinality, data type, and numerical ranges
when it applies. This module creates a summary profile per field, \ie column or
attribute, in the original data, but this granularity can be fine-tuned as
required.
\item {\bf Graph builder.} We create search paths over the signatures that
reflect different relatedness measurements and allows one to navigate the
sources, such as similarity measures between data sources. Such similarity
measures may refer to the content, the schema, the semantic type, but also to
the hierarchy in which data sources were read, \ie if they are part of the same
schema. Creating these search paths require in general a pairwise comparison
that would render the module too slow.  We discuss next some techniques we use
to tame the scale.  
\end{enumerate}

Once the graph has been built, it can be accessed by a set of data
discovery primitives that can be used interactively to explore and find relevant
data by both users and other modules, such as stitching. Data discovery primitives can be composed
into more complex data discovery functions, and all of these combine through
combinator operators to build expressive discovery queries.  

\subsection{Data Profiler}

The profiling and summarization module is responsible for computing signatures
for each attribute in the dataset. A signature for finding similar attributes
must: (i) represent the attribute values in a compact way; and (ii) be
compatible with a similarity metric. 

Currently we have primitives for numerical and textual values. For numerical
values the module learns the probability distribution of the data. We use
non-parametric methods for this, as it is in general impossible to tune the
learning process for every attribute in the dataset due to the scale. We use
kernel density estimators for numerical values.  For textual values, we first
represent each attribute as a bag of words and then compute its TF-IDF vector.

\textbf{Data quality estimation.} As part of the profiling process and aside
from the signature extraction, the component is also responsible for detecting
data type, cardinality and performing some basic denoising to avoid common data
errors. All this information serves as a data quality estimation for other
modules in Data Civilizer, such as stitching and the query processor \ref{sec:4,
sec:5}). 

For efficient cardinality and quantile estimation, required to summarize
numerical data, we use sketches extensively. This allows for memory-efficient
computation. By treating the input data as a read-once stream we also avoid
throttling the original data sources and in general, allows higher performance
summary computation.

%\sibo{The details for entity extracting / sematic type analysis are missing.}
%
%To extract entity / analyze semantic type for textual data, we use {\em named
%entity recognizers (NER)} \cite{XXX} to detect typical semantic types, e.g.,
%person, date, and location. However, the cost to invoke the NERs is extremely
%high, and it is unfavorable to apply NERs to every value in the column. As an
%alternative, we adopt an early termination approach that stops the NER checking
%as soon as the module is confident about the semantic type, e.g., consistently
%identifying the same semantic type with new fed data in the column. This allows
%much higher performance without significantly scarifying the entity extraction /
%semantic type analysis accuracy.

%\input{sections/5-cleanliness_estimation}

\subsection{Graph Builder} The input of the graph builder is a set of profiles,
that are represented as nodes in a multigraph. The goal of the builder component
is to find content and schema similarity relationships among these nodes, which
are then represented as edges. In this way, the multigraph will have an edge of
type \emph{content-similarity} between nodes A and B if these are similar, and
one of type \emph{schema-similarity} if the schema names are similar too. Each
edge is scored with a weight that represents the strength of the relationship:
the particular meaning depends on the edge semantics.

Although this approach creates a logically complete graph, in practice it is
possible to define a minimum similarity threshold to prune edges. Building the
graph requires a pair-wise operation among all attributes in the
entire dataset, a $N^2$ operation. To scale this process, we are introducing
approximate nearest neighbor techniques based on locality sensitive hashing \cite{DBLP:conf/compgeom/DatarIIM04}
that helps to scale the process. In addition, we have built a distributed
prototype that can ingest data and compute signatures and search paths in
parallel.


