% !TEX root = ../main.tex
\section{Discovery}
\label{sec:discovery}

The data discovery module's goal is to find relevant data from perhaps hundreds
to thousands of data sets  spread across the different storage systems of an
enterprise.

Current solutions to finding relevant data include asking an expert (if one is
available) or  performing manual exploration by inspecting data sets one by one.
Obviously, both approaches are time-consuming and prone to missing relevant data
sources. Recently, there have been efforts to automate this process, some
examples are Goods~\cite{DBLP:conf/sigmod/HalevyKNOPRW16} and
InfoGather~\cite{DBLP:conf/sigmod/YakoutGCC12}. Goods permits users to inspect
datasets and find others similar to datasets of interest. Infogather permits to
extend the attributes of a dataset of interest with attributes from other
datasets. These systems are designed to solve these particular use cases, and so
their indexes are built for this specific purpose. In
contrast, our discovery module aims to support general \emph{discovery queries} that
can then be used for unanticipated discovery needs. Hence, the discovery module
uses a linkage graph (Section \ref{subsec:graphbuild}) to permit a broader range
of queries. 

%Mourad: A bit handwaving!

%\srm{I don't understand the
%following sentence, vis-a-vis the Goods paper, which seems to be about
%supporting data discovery for a bunch of use cases, without one specific
%use-case in mind.}\mourad{I agree with Sam; after skimming through the paper, I
%see Goods as a serious competitor to DC, at least on the discovery side.}
%
%Unlike these systems that are designed with a specific use-case in mind, our
%discovery module aims to collect enough information about the data sets and to
%expose them to upper layers of \dcv, allowing the orchestration of complex query
%workflows that integrate a variety of cleaning, linking, and querying
%operations.

%execution, which is the ultimate goal.  

The data discovery module narrows down the search for relevant data from
the total number of data sources to a handful, on which finer-grained
relationships can be computed efficiently. \dcv discovery supports a
user-level API to search for relevant data using a variety of techniques such as
searching for tables with specific substrings in the schemas ({\it schema
search}) or table values ({\it content search}). 
%Discovery is extensible; an engineer can extend the supported relationships to
%cover other relevant use cases.

\subsection{Data Profiling at Scale}\label{subsec:profile}

The key idea is to summarize each column of each table into a {\em profile}.
A profile consists of one or more {\it signatures};  each signature summarizes
the original contents of the column into a domain-dependent, compact
representation of the original data.  By default, signatures for numerical values consist of a
histogram representing the data distribution and; for textual values they consist of a vector
of the most significant words in the column.  Our profiles also
contain information about data cardinality, data type, and numerical ranges if
applicable~\cite{profiling_survey}.

%Note that finding pairwise relationships between each attribute profile is an
%$O(n^2)$ computation, which will not scale.  We thus rely on locality sensitive
%hashing (LSH)~\cite{DBLP:conf/compgeom/DatarIIM04} to address this issue.
%Specifically, we hash each signature and group together the colliding ones.
%%those that % collide.  These are more likely to be similar; we only need to
%perform comparisons among the nodes in each cluster, ignoring cross cluster
%comparisons.  This approach avoids $O(n^2)$ operations and appears to perform
%well in practice.

%\subsection{Data Discovery Components}
%
%The data discovery module consists of two conceptually different components, a
%data profiler and a graph builder, that collaborate to build the linkage graph
%indicated in Figure~\ref{fig:arch}. 
%
%\begin{enumerate}
%\item {\bf Data profiler.} This component summarizes each column of each table
%into a profile. Each profile contains a signature, which is a domain-dependent,
%compact representation of the original content.  For numerical data, we use the
%data distribution, and for textual data,  a vector of the most significant words
%in the column.  Our profiles also contain information about data cardinality,
%data type, and numerical ranges if applicable~\cite{profiling_survey}.
%
%\item {\bf Graph builder.} Using the profiles, the graph builder finds
%relationships among nodes and represents them as edges. Examples of
%relationships are \emph{content similarity} based on measuring the similarity
%distance between signatures and \emph{schema similarity} that captures the
%similarity of the labels of the different nodes. Another kind of relationship
%captures the hierarchical relationship between a table and its columns.
%\nan{Each node is a column, or a table? The graph is not formally defined.}
%\sibo{The definition of node appears in Section \ref{introduction}, but it will
%be better to recap the definition again.}
%
%\end{enumerate}
%
%The graph resulting from this process can be used to answer discovery queries.
%Discovery queries vary from simple keyword search, useful to quickly glance over
%data, to more involved queries such as table augmentation, to add attributes of
%interest to known tables. We discuss the queries at the end of the section. 

%\subsection{Data Profiling at Scale}

To run at scale, discovery relies on sampling to estimate the cardinality of
columns and the quantiles for numerical data. This avoids having to sort each
column, which is not a linear computation.  This also allows us to avoid making
a copy of the data,  reducing memory pressure.

The profiler consists of a pipeline of stages. The first stage
performs basic de-noising of the data, such as removing empty values and dealing
with potential formatting issues of the underlying files, \eg figuring out the
separator for a CSV file.  The second stage determines the data type of each
column, information that is propagated along with the data to the next stages.
The rest of the stages are responsible for computing cardinalities, and performing
quantile estimation, \eg to determine numerical ranges.

The profiler has connectors to read data from different data sources, such as
HDFS files, RDBMS or CSV files. Data sets are streamed through the profiler
pipeline;  the pipeline outputs the results
%and  at the end the resulting attribute profile is sent
to a {\it profile store} where it is later consumed by the graph builder, which
will be explained in Section \ref{subsec:graphbuild}.
%At the same time,
The profiler works in a distributed environment using a lightweight coordinator
that splits the work among multiple processors, helping to scale the
computation.

%The discovery module benefits from our distributed architecture that allows us
%to parallelize profiling as well as graph building, further speeding up
%execution times. Other kinds of relationships such as finding FK-PKs are too
%expensive to compute at scale for the whole graph.  These are left to the
%stitching module, which is described next.

\subsection{Discovery Queries}
\label{subsec:api}

The profiles are the building blocks used by the graph builder (Section
\ref{subsec:graphbuild}) to produce a linkage graph, that in turn serves as the
main data structure to answer discovery queries. Users can submit
\emph{discovery queries} to find data sets whose schemas are relevant to the
task at hand. For example, to find all data sets that contain names of employees
in a company, the user issues a discovery query that searches for all attributes
referring to an employee or a name. The user can then write a SQL query to filter
from a list. Here are some examples of functions that the discovery module
provides to users:
%\srm{I think a little more detail is
%needed here describing how the profiles built above are used to answer these
%queries.  As it is the two subsections don't seem well connected -- why are
%profiles needed?} 

\begin{itemize}
\item \textbf{Fill-in schema}. Given a set of names, this function retrieves 
tables or groups of tables that contain the desired attributes. The core
of the operation consists of finding tables with similar attribute names to the
provided ones.
%\mourad{How do you know?}

\item \textbf{Extend attribute.} This is similar to the extend operator of
Octopus~\cite{DBLP:journals/pvldb/CafarellaHK09}, or the ABA operation of
Infogather~\cite{DBLP:conf/sigmod/YakoutGCC12}. Using this function users can
extend tables of interest with additional attributes. The core of the operation
consists of finding matches to the current table, and then retrieving 
attributes that do not appear in the original table.

\item \textbf{Subsumption relationships.} Given some reference table, this
function provides a list of tables or groups of tables that have some form of
subsumption relationship (\ie are contained in or contain) with respect to the
reference table. 

\item \textbf{Similarity and lookup.} Discovery can also be used to find schemas
that contain data similar to some provided schema (attribute or table), as well
as to find schemas that contain certain values, \ie keyword search for textual
data and range search for numerical data.  

\end{itemize}

Discovery works similarly to an information retrieval system: schema retrieval queries
do not return a specific answer, but instead return a ranked set of results. We
are currently exploring several ranking options, and a model that decouples
discovery query processing from ranking evaluation seems the most promising.

%\srm{Any suggestions about how to do this?}

