% !TEX root = ../main.tex
\section{Discovery}
\label{sec:discovery}

The goal of the data discovery module is to find relevant data among the
millions of datasets spread across many storage systems of modern organizations.
Suppose an analyst wants to answer the question: what is the monthly sales trend
by department? The analyst knows conceptually what data is needed to answer this
question (a table of sales, a table of departments, a table of products sold by
each department), but not which specific data sources (which relations in a
schema or what files in an HDFS deployment) contain such data. The typical
solution is to 1) ask an expert (if such a person is available), or to perform
manual exploration, inspecting datasets one by one (which is time-consuming and
prone to missing relevant data sources). We say this analyst is facing a data
discovery challenge.

The data discovery module narrows down the search of relevant data from the
thousands and millions of data sources to a handful of them that can then be fed
to stitching---that performs the final preparation before processing. Discovery
exports an API that can be used by users directly to search for relevant data,
e.g., schema search, similar content, etc, as well as by other modules in Data
Civilizer, such as stitching, to retrieve additional data that can be used, for
example, to enrich a table. Next we describe the general steps of data
discovery:

\subsection{Data Discovery Components}

The data discovery module consists of two conceptually different components that
collaborate with each other to build a skeleton of the linkage graph shown in
Figure~\ref{fig:arch}. The linkage graph is defined as $G=(V,E)$, where $E$ is a
multiset, as we permit different types of edges, \ie relationships, betweeen
$V$. To build this multigraph, discovery consists of two modules, the first one,
the \code{data profiler} is in charge of finding the $V$ in the graph, and the
second one, the \code{graph builder} is reponsible for finding $E$.

\begin{enumerate}
\item {\bf Data profiler.} We first accumulate knowledge of the data sources by
summarizing them into concise profiles. We can choose the granularity of those
profilers. In practice, we have found that building a profile per attribute is
sufficient for most use cases. A profile contains a signature, which is a
domain-dependent, compact representation of the original content. One example of
signature for numerical data is its distribution, and for textual data a vector
with the most significative terms. A profile also contains information about the
data cardinality, data type, and numerical ranges when it
applies~\cite{profiling_survey}.  

\item {\bf Graph builder.} The data profiler generates a set of $V$, and the
graph builder reponsibility is to find relationships among these, and represent
them as $E$, building the skeleton of the linkage graph. These relationships
help to navigate through the different sources ($V$). Examples of relationships
that the graph builder finds are \emph{content similarity} based on measuring
the similarity distance among the signatures that represent each $v$. Another
relationship is \emph{schema similarity} that captures the similarity of the
names of the different $v$. Other kind of relationships capture the hierarchical
relationship among $v$, for example, all attributes of a same relation are
connected, permitting quick navigation. Creating these relationships require in
general a pairwise comparison that would render the module too slow. We discuss
next some techniques we use to tame the scale.
\end{enumerate}

Once the linkage graph skeleton has been built, it can be accessed by a set of
data discovery primitives to explore and find relevant data. These primitives
are used by users and by other modules, such as stitching. In particular,
stitching also has write access to the linkage graph, that uses to add a new $e$
to the multiset of relationships, the expensive-to-compute FK-PK relationships.
Data discovery primitives can be composed into more complex data discovery
functions, and all of these combine through combinator operators to build
expressive discovery queries.  

\subsection{Data Profiler}

The profiling and summarization module is responsible for computing signatures
for each attribute in the dataset. A signature for finding similar attributes
must: (i) represent the attribute values in a compact way; and (ii) be
compatible with a similarity metric. 

Currently we have primitives for numerical and textual values. For numerical
values the module learns the probability distribution of the data. We use
non-parametric methods for this, as it is in general impossible to tune the
learning process for every attribute in the dataset due to the scale. We use
kernel density estimators for numerical values.  For textual values, we first
represent each attribute as a bag of words and then compute its TF-IDF vector.

\textbf{Data quality estimation.} As part of the profiling process and aside
from the signature extraction, the component is also responsible for detecting
data type, cardinality and performing some basic denoising to avoid common data
errors. All this information serves as a data quality estimation for other
modules in Data Civilizer, such as stitching and the query processor \ref{sec:4,
sec:5}). 

For efficient cardinality and quantile estimation, required to summarize
numerical data, we use sketches extensively. This allows for memory-efficient
computation. By treating the input data as a read-once stream we also avoid
throttling the original data sources and in general, allows higher performance
summary computation.

%\sibo{The details for entity extracting / sematic type analysis are missing.}
%
%To extract entity / analyze semantic type for textual data, we use {\em named
%entity recognizers (NER)} \cite{XXX} to detect typical semantic types, e.g.,
%person, date, and location. However, the cost to invoke the NERs is extremely
%high, and it is unfavorable to apply NERs to every value in the column. As an
%alternative, we adopt an early termination approach that stops the NER checking
%as soon as the module is confident about the semantic type, e.g., consistently
%identifying the same semantic type with new fed data in the column. This allows
%much higher performance without significantly scarifying the entity extraction /
%semantic type analysis accuracy.

%\input{sections/5-cleanliness_estimation}

\subsection{Graph Builder} The input of the graph builder is a set of profiles,
that are represented as nodes in a multigraph. The goal of the builder component
is to find content and schema similarity relationships among these nodes, which
are then represented as edges. In this way, the multigraph will have an edge of
type \emph{content-similarity} between nodes A and B if these are similar, and
one of type \emph{schema-similarity} if the schema names are similar too. Each
edge is scored with a weight that represents the strength of the relationship:
the particular meaning depends on the edge semantics.

Although this approach creates a logically complete graph, in practice it is
possible to define a minimum similarity threshold to prune edges. Building the
graph requires a pair-wise operation among all attributes in the
entire dataset, a $N^2$ operation. To scale this process, we are introducing
approximate nearest neighbor techniques based on locality sensitive hashing \cite{DBLP:conf/compgeom/DatarIIM04}
that helps to scale the process. In addition, we have built a distributed
prototype that can ingest data and compute signatures and search paths in
parallel.


