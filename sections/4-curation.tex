% !TEX root = ../main.tex
\section{Polystore Query Processing}
\label{sec:curating}

\dcv uses the \texttt{BigDAWG}
polystore~\cite{DBLP:journals/pvldb/ElmoreDSBCGHHKK15} to federate access to
multiple storage systems inside an organization. \texttt{BigDAWG} consists of a
middleware query optimizer and executor, and shims to various local storage
systems. We assume that a user has run discovery and that the corresponding
linkage graph has been computed.  If the user is interested in a composite table
containing a particular set of columns that are not available in any single data
source,  we can use the PK-FK mining techniques described in the previous
section to find a set of possible join paths that can be used to materialize the
table of interest to the user. Furthermore, for any particular join path, it is
straightforward to construct the \texttt{BigDAWG} query that materializes the
view specified by each join path. However, we still need to choose which of
several possible join paths is the best to compute the table of interest to the
user.
	

% in Section~\ref{subsec:selectview}. As one of the most important criteria to select the join path is its cleanliness, we then propose a model to estimate the cleanliness of the join path in Section~\ref{subsec:model}. When the join path is selected, we need to clean it with a given budget. We develop a method to integrate the cleaning operations in the query plan that materializes the view in Section~\ref{subsec:gain}.


\subsection{Join Path Selection}

The conventional data federation wisdom is to choose the join path that minimizes the query processing cost. However, this ignores data cleaning issues, to which we now turn. To achieve high quality results, one has to clean the data in conjunction with querying the table. For example, if one has a data value, \code{New Yark}, and wants to transform it to one of its airport codes \code{(JFK, LGA)}, then one must correct the data to \code{New York}, prior to the airport code lookup. 
Cleaning usually entails a human specification of the correct value or a review of the value produced by an automatic algorithm. 
Such human costs can easily dominate the total query execution time and represent a significant financial cost in many data cleaning scenarios.
As such, one goal of \dcv is to choose the join path that produces the highest quality answer, rather than the one that is easiest to compute. 

In \dcv, a user has to decide how to trade off data quality and cleaning cost. 
\dcv defines two parameters, both under the user's control.

\begin{enumerate}
\item Minimize cost for a specific cleanliness metric. In this case, the user requires the data to be a certain percentage, \emph{P}, correct and will spend the least it takes to get to that point.

\item Maximize accuracy for a specific cost. In this case, the user is willing to spend \emph{M} and wishes to make the data as clean as possible.
\end{enumerate}

Sometimes the user is the one actually cleaning the data. In this case, she can use \emph{P} and \emph{M} to quantify the value of her time. 
In other cases, cleaning is performed by other domain experts, who generally need to be paid. In this case, \emph{P} and \emph{M} are statements about budget priorities.

Sometimes the user may prefer the join path that yields the largest view size, \ie the number of rows in the view. 
For example, the user may want a view with 1,000 rows and 90\% cleanliness rather than a view with 10 rows and 98\% cleanliness. 
We can combine all these facets as the criteria for join path selection.
Estimating result set sizes has been extensively studied in the literature~\cite{DBLP:conf/sigmod/IoannidisP95} and we just utilize the cardinality estimates produced by \texttt{BigDAWG}.

As a result, \dcv must make the following decisions. 
First, it needs to assess the cleanliness of the result of any given join path. We discuss this issue in Section~\ref{subsec:model}. 
Then, we need to choose where to place, in the resulting query plan, data cleaning operations to be the most efficient. This is the topic of Section~\ref{subsec:gain}.


\subsection{Cleanliness Model}
\label{subsec:model}

To assess the cleanliness of join paths and eventually full queries, 
%In \dcv, we collect information about the errors in each data set. Specifically, 
we need to devise an accuracy metric for each cell, namely an estimate for the probability of the cell to be erroneous. To this end, \dcv first applies automatic error detection tools and then uses a composite metric to accumulate detected errors. 

While there are several types of data errors that we could consider, in \dcv we limit to the following three types:

\stab
(i)~Outliers -- these are data values that are distant from the distribution of values in the same column. 

\stab
(ii)~Duplicates -- these are groups of tuples that refer to the same real-world entity. The mismatching values between these duplicate tuples is considered to be an error.

\stab
(iii)~Integrity Constraint (IC) Violations -- these include functional dependency (FD) violations and inclusion dependency (IND) violations. There is a violation between two tuples in a table $\R$ of an FD $\varphi: R(X\rightarrow Y)$ if their values on the LHS of $\varphi$ are the same while their values on the RHS are not. There is a violation between tuples in tables $\R$ and $\S$ of an IND: $\R[X]\subseteq\S[Y]$ if a value on the LHS does not exist in the RHS. To detect more errors, we can further conduct schema mapping, merge semantically equivalent tables, and detect the IC violations in the merged table.

\stab
We have recently studied several representative data cleaning systems on a collection of real world data sets ``from the wild''~\cite{DBLP:journals/pvldb/AbedjanCDFIOPST16} and we found out that there was no cleaning Esperanto. Hence multiple tools are required to achieve reasonable accuracy and point solutions neither offer enough functionalities nor achieve acceptable performance. Therefore an ensemble approach is required. For this purpose, we model the errors detected from multiple tools (we use one tool for each of the above error category) as a conflict hypergraph~\cite{DBLP:conf/icde/ChuIP13} and compose an overall cleanliness score for each cell. Next we discuss how to estimate the cleanliness of a join path, based on the cleanliness estimation of each cell participating in the join.

For each join path, we first construct a query corresponding to this join path and execute it. 
Then we extend the techniques in data lineage~\cite{DBLP:conf/cidr/Widom05} to find the cell level lineage $\lambda$. 
Given a cell in the result, $\lambda$ indicates all the cells in the source table that can affect the probability of the result cell to be clean. 
To obtain the lineage $\lambda$, 
%for each selection and join predicates, %Mourad: Selection predicates are coming from nowhere. 
we propagate for each join predicate the cells in the attributes that participate in the predicate to the other cells within the same tuple. 
Then for each result cell, we multiply the cleanliness values of all the source cells in its lineage to get its cleanliness. In this way, we can propagate the cleanliness of the source cells to the result cells.

The cleanliness of a join path is measured by the average cleanliness of all the cells in the result. \dcv selects the join paths based on the criteria that linearly combines the sizes and the cleanlinesses of their corresponding query results. Note that \dcv also uses schema mapping~\cite{DBLP:conf/cidr/StonebrakerBIBCZPX13} to cluster the join paths based on their semantics and ranks the join paths within each categories for users to choose.


%An extension of the above majority voting methodology is to construct a ``disorder metric'' that indicates the distance between an incorrect value and its ground truth. For numeric values, we use the average and standard deviation as the disorder metric. For example, if salary errors average 5\% with standard deviation 2\%, then most of the errors are in the range from 3\% to 7\%. For text values, we use the ``lexical distance'' (such as distance based on WordNet~\cite{WordNet,DBLP:journals/cacm/Miller95} and edit distance) as the disorder metric. For example, if an address field routinely confuses ``\textit{road}'' and ``\textit{street}'', but very rarely gets the name of the street wrong, then the lexical distance is very small. Using this tactic, we can use fuzzy predicates and fuzzy joins in place of the exact operations to improve the accuracy of our results.
%We plan to examine this tactic in detail as future work. We now propose a mechanism to estimate the cleanliness of a join path using the error information.


%We first define the cleanliness of an edge in the join graph. We first consider the join on textual values. Consider two linked nodes \RX and \SY in the graph, where we align the values in \RX and \SY using the maximum matching defined in Section~\ref{subsec:eind}. Suppose that the maximum lexical distances in the two nodes are respectively \Dis(\RX) and \Dis(\SY). Hence, the distance between two values in \RX and \SY that represent the same object should be within $\Dis(\RX)+\Dis(\SY)$. Thus we limit the distance between the pairs of aligned values in the maximum matching to $\Dis(\RX)+\Dis(\SY)$. The fraction of the values in the foreign key that appear in the maximal matching is an estimate for the cleanliness of the edge. Similarly, we can do the same  for numeric values with a numeric metric.
%Given a join path, its cleanliness is obtained by multiplying the cleanliness values of all the edges and the nodes involved in the predicates.

%}


\iffalse
Specifically, we need an accuracy metric for each column, namely an estimate for the percentage of the column which is erroneous. There are multiple ways to achieve this goal including: 
(i)~Have the DBA who added the table make an estimate for the accuracy of each column. 
(ii)~Estimate the fraction of null values and outliers in the column as $e$, and assume that a second accuracy estimator is $1-e$.
(iii)~Conduct a similarity join on any foreign key and the primary key of the table and assume any mismatched values are errors. We can also assume that these error estimates can be used for the non-key fields in the table. 
(iv)~Cluster the tables with similar primary keys and assume that their records have the same semantic information.  Therefore, we identify the columns in the cluster which are similar, and use majority voting to estimate the cleanliness of each column. 
All of these methods can generate the  error estimates we need. 
Hence, we can linearly combine these estimates based on our confidence in the four methods. For example, we can assign a high coefficient to the first method if the DBA is trustworthy. This gives us a mechanism to assign a cleanliness estimate to any non-key attribute. These cleanliness estimations can be stored and updated in the linkage graph.

An extension of the above majority voting methodology is to construct a ``disorder metric'' that indicates the distance between an incorrect value and its ground truth. For numeric values, we use the average and standard deviation as the disorder metric. For example, if salary errors average 5\% with standard deviation 2\%, then most of the errors are in the range from 3\% to 7\%. For text values, we use the ``lexical distance'' (such as distance based on WordNet~\cite{WordNet,DBLP:journals/cacm/Miller95} and edit distance) as the disorder metric. For example, if an address field routinely confuses ``\textit{road}'' and ``\textit{street}'', but very rarely gets the name of the street wrong, then the lexical distance is very small. Using this tactic, we can use fuzzy predicates and fuzzy joins in place of the exact operations to improve the accuracy of our results.
We plan to examine this tactic in detail as future work. We now propose a mechanism to estimate the cleanliness of a join path using the error information.


We first define the cleanliness of an edge in the join graph. We first consider the join on textual values. Consider two linked nodes \RX and \SY in the graph, where we align the values in \RX and \SY using the maximum matching defined in Section~\ref{subsec:eind}. Suppose that the maximum lexical distances in the two nodes are respectively \Dis(\RX) and \Dis(\SY). Hence, the distance between two values in \RX and \SY that represent the same object should be within $\Dis(\RX)+\Dis(\SY)$. Thus we limit the distance between the pairs of aligned values in the maximum matching to $\Dis(\RX)+\Dis(\SY)$. The fraction of the values in the foreign key that appear in the maximal matching is an estimate for the cleanliness of the edge. Similarly, we can do the same  for numeric values with a numeric metric.
Given a join path, its cleanliness is obtained by multiplying the cleanliness values of all the edges and the nodes involved in the predicates.
\fi



%In \dcv, we collect information about the errors in each data set. We first describe the two kinds of error information we collect. First, we need the accuracy metrics for each column, namely an estimation for the percentage of the column which is erroneous. Second, we need a ``disorder metric'' that indicates the distance between an incorrect value and its ground truth. For numeric values, we use the average and standard deviation as the disorder metric. For example, if salary errors average 5\% with standard deviation 2\%, then $\frac{2}{3}$ of the errors are less than 7\%. \footnote{\sibo{Where the 2/3 comes from? The distribution is not given in the context}}. For text values, we use the ``lexical distance'' (such as distance based on WordNet~\cite{WordNet,DBLP:journals/cacm/Miller95} and edit distance) as the disorder metric. For example, if an address field routinely confuses ``\textit{road}'' and ``\textit{street}'', but very rarely gets the name of the street wrong, then the lexical distance is very small. %\srm{We should explicitly define different metrics for lexical and numeric.  Mean +/- std dev doesnâ€™t make sense for lexical, I think.}


%Next we discuss how to collect the error information. There are multiple ways to achieve this and they can be composed. First, we can have the DBA who added the table make an estimate for the error information. Second, we can estimate the number of null values and outliers in the column as errors. Third, we can conduct a similarity join on the foreign key and primary key, take the mismatching values as errors, and use this error information for the other columns. Fourth, we can cluster the tables with similar primary key such that their records may have the same semantic, identify the same columns in the cluster, and use majority vote to estimate the cleanliness of each column. We can also have the DBA give the textual semantic description of the table as well as each column in the table to refine the table clustering and common column finding. All of the four methods above can generate the two error estimations we needed. We can linearly combine these estimations based on the confidence of the four methods. For example, we can assign a high coefficient to the first method if the DBA is trustworthy. Next we discuss propose a cleanliness model to estimate the cleanliness of a join path using the error information.


%To answer a query, \dcv generates a sub-graph for the query from the available join graph (which is the sub-graph of the linkage graph that only contains FK-PK edges) that covers all the joins in the query. For a given query, there may be multiple sub-graphs and we provide a multi-faceted interface for the user to pick one. One facet is the one with maximum cleanliness. Next we discuss how to achieve this according to the following cleanliness model.


% We first define the cleanliness of an edge in the join graph. We first consider the join on textual values. Given two linked nodes \RX and \SY in the graph, when we align the values in \RX and \SY using the maximum matching defined in Section~\ref{subsec:eind}, we can tolerate some errors. Suppose that the maximum lexical distances in the two nodes are respectively \Dis(\RX) and \Dis(\SY). The distance between two values in \RX and \SY that represent the same object should be within $\Dis(\RX)+\Dis(\SY)$. Thus we limit the distance between the pair of aligned values in the maximum matching by $\Dis(\RX)+\Dis(\SY)$. Then the fraction of the values in the foreign key that appear in the maximal matching is the cleanliness of the edge. Similarly, we can do the same thing on numeric values with numeric metric.

% We define the cleanliness of a node \RX simply as its accuracy \Acc(\RX), which means the percentage of values in \RX that are not erroneous.

% Given a join path, its cleanliness is obtained by multiplying the cleanliness values of all the edges, and the nodes involved in predicates which is necessary since errors can propagate along the edge and is dominated by the dirtiest one.



\subsection{Query Planning with Data Cleaning}
\label{subsec:gain}


In a query plan with cleaning operations, the cleaning cost and the result quality are different based on the position of the data cleaning operation. For example, cleaning a whole column before the selection on this column yields a higher cleaning cost and query accuracy than putting the cleaning operation after the selection operation. Obviously expensive cleaning should be performed on as few records as possible and have as most impact as possible. 
%Thus, we need a way to obtain a query plan with cleaning operations that achieve high quality gain with a limited cleaning budget. 
In \dcv, we assume that given a query and the cleanliness estimation value on each result cell we are give a set budget $B$ of user's actions to check and eventually change a value if it is erroneous.
Based on this input, we need to determine which cells need to be sent to the user for feedback in order
to achieve the highest quality gain.

More specifically,
%The intuition of our solution is that \dong{we 
we first execute the query and obtain its result. 
We then map the cells in the query result back to the cells in the source tables using the data lineage as discussed in the previous section. 
To achieve the maximum quality and be able to reuse user's cleaning efforts, 
we focus only on the source cells that are involved in the erroneous result cells. As each source cell can contribute to multiple erroneous result cells, for each source cell, we accumulate its impact on the result and clean them in the decreasing order of their impacts until the budget is exhausted. This is not only effective in cleaning the current join path results, but also help enhance the quality of the whole system and subsequent queries.


%Another better way is that we use dynamic programming to obtain a query plan with cleaning operations that achieve the maximum accuracy gain with limited cleaning cost budget or achieve the desired accuracy with the smallest cleaning cost budget. 






\iffalse
For example, if we run a query with the following predicate:

\vspace{.5em}
\dots \textsf{where} $name = $ ``\code{New York}''
\vspace{.5em}



\noindent Then the misspelled city name,  \ie \code{New Yark}, will not be found, and accuracy will suffer. One solution is to clean the entire source data sets to avoid such errors, an expensive proposition indeed. If the budget is small, then we can insert cleaning ``late'' in the query plan. On the other hand, if the penalty is large, then we must insert cleaning earlier, even though at much higher cost. 

\fi


\iffalse
For each edge in the join path, if we put the cleaning operation ahead of the join operation, i.e., we only clean the tuple pairs in our maximal matching, we spend cleaning cost linear to the number of foreign keys and can only expect to achieve the same accuracy  as the cleanliness of the edge. In contrast, if we first clean all the possible tuple pairs of the foreign key and primary key and then generate the real join results, we can achieve 100\% accuracy with a cleaning cost quadratic to the number of foreign keys. Similarly, if we put the cleaning operation behind the where clause, we can only expect to achieve the same accuracy of the column with the cost proportional to the number of tuples satisfying the where clause. Otherwise, we can achieve 100\% accuracy with the cost proportional to the number tuples in the whole column.


In this setting, we can use dynamic programming to obtain a query plan with cleaning operations that achieve the maximum accuracy gain with limited cleaning cost budget or achieve the desired accuracy with the smallest cleaning cost budget. More specifically, for each node and edge in the join graph, we can choose not to clean it, clean it before the query operation (i.e., join or selection), or clean after the query operation. Each with different cleaning cost. With dynamic programming, we can achieve an optimal strategy.
\fi
