% !TEX root = ../main.tex
\section{Curating Polystore}
\label{sec:curating}

Our local polystore system is called BigDAWG.  It consists of a middleware query
optimizer and executor, and shims to various local storage systems, as noted in~\cite{DBLP:journals/sigmod/DugganESBHKMMMZ15, DBLP:journals/pvldb/ElmoreDSBCGHHKK15}.  Assume that a user has run discovery and stitching to identify
a virtual relation (view) that he wishes to query.  Also, assume that he has
identified the subset of each source table in which he is interested.  In other
words, he has defined the join path using stitching and the predicates that
subset the tables in some other way.  This will translate directly to a BigDAWG
query.  In general, a user will want to retrieve a subset of the view, which is
merely an extra predicate(s) attached to the BigDAWG query.   In this section we
address how to integrate data cleaning and transformation operations with this
query plan.

In general, one needs to clean the data prior to performing transformations.
For example, if one has a data value, New Yark, and wants to transform it to one
of its airport codes (JFK, LGA), then one must correct the data to New York,
prior to the airport code lookup.  Obviously, cleaning usually entails a human
specification of the corrected value or a review of an automatic algorithm.
Hence, it is expensive in human resources, which we believe is generally ``the
high pole in the tent''.  As such, a user has to decide how he wants to trade off
data cleanliness and cost.  Data Civilizer defines two parameters under his
control.

\begin{enumerate}
\item Minimize cost for a specific cleanliness metric.  In this case, the
user requires the data to be a certain percentage, P, correct and will spend
whatever it takes to get to the point.
\item Maximize accuracy for a specific cost.  In this case, the user is
willing to spend M and wishes to make the data as clean as possible.
\end{enumerate}

Sometimes the user is the one actually cleaning the data.  In this case, he can
use P and M to quantify the value of his time.  In other cases, cleaning is done
by other domain experts, who generally need to be paid.  In this case, P and M
are statements about budget priorities.

Obviously one want to perform expensive cleaning on as few records as possible.
Hence, one would like to insert cleaning operations as late in the query plan as
possible.  Unfortunately, if one runs a query that has the predicate:

…where name = “New York”

then the misspelled city name will not be found, and accuracy will suffer.  One
solution is to clean the entire source data sets to avoid such errors, an
expensive proposition indeed.  In Data Civilizer, we support a third
alternative; namely more information about the errors in each data set.  First,
we assume that each data set owner gives us accuracy metrics for each column,
namely an estimate for the percentage of the column which is erroneous.  Second,
the same data set owner is required to specify a “disorder metric”, which
indicates the average and variance of the lexical distance between an incorrect
value and its ground truth.  For example, if salary errors average 5% with
variance 2%, then 2/3 of the errors are less than 7% off.  If an address field
routinely confuses “road” and “street”, but very rarely gets the name of the
street wrong, then the lexical distance is again very small.   If the penalty is
small, then we can insert cleaning “late” in the query plan.  On the other hand,
if the penalty is large, then we must insert cleaning earlier, of course at much
higher cost.  The next section analyzes this tradeoff in more detail.

 
