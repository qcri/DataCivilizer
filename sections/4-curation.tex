% !TEX root = ../main.tex
\section{Polystore Query Processing}
\label{sec:curating}

\nan{
(i) what is the view we want to compute, in a conceptual level?
(ii) why cleaning is the main objective of view selection?
(iii) is Polystore that important since it is only mentioned in the first paragraph of Section 4?
}

\dcv uses the BigDAWG polystore~\cite{DBLP:journals/pvldb/ElmoreDSBCGHHKK15}. 
BigDAWG  consists of a middleware query optimizer and executor, and shims to various local storage systems.
%, as noted in~\cite{DBLP:journals/sigmod/DugganESBHKMMMZ15,DBLP:journals/pvldb/ElmoreDSBCGHHKK15}. 
We assume that a user has run discovery and that the corresponding linkage graph has been computed. 
We can thus identify a collection of join paths that can materialize a composite table of interest to the user.  
Also, assume that the user has identified the subset of each source table in which he is interested. 
For any join path, it is now straightforward to construct the BigDAWG query that materializes the view specified by each join path. 

\subsection{Selecting a View to Materialize}



The conventional data federation wisdom is to choose the join path that minimizes the query processing cost. However, this ignores data cleaning issues, to which we now turn. To achieve high quality results, one has to clean the data prior to querying the table. For example, if one has a data value, \code{New Yark}, and wants to transform it to one of its airport codes \code{(JFK, LGA)}, then one must correct the data to \code{New York}, prior to the airport code lookup. Obviously, cleaning usually entails a human specification of the corrected value or a review of the value produced by an automatic algorithm. Hence, it is expensive in human resources, which we believe is generally ``the high pole in the tent''. As such, one goal of \dcv is to choose the join path that produces the highest quality answer and not the one that is easiest to compute. However, sometimes the users may prefer the join path with largest result size, i.e., largest number of rows in the view. As such, we provide a multi-faceted method which can computes multiple join paths with different criteria  and leave the choice of the criteria to the users. \footnote{Previous comments: \nan{Our solution for selecting a view to materialize is not well justified.}}\footnote{Previous comments: \srm{Don’t I care about completenetss of match as well?}}


In \dcv, a user has to decide how to trade off data quality and cleaning cost. 
\dcv defines two parameters under the user control.

\begin{enumerate}
\item Minimize cost for a specific cleanliness metric. In this case, the user requires the data to be a certain percentage, \emph{P}, correct and will spend whatever it takes to get to that point.

\item Maximize accuracy for a specific cost. In this case, the user is willing to spend \emph{M} and wishes to make the data as clean as possible.
\end{enumerate}

%\sibo{More facets probably come here.}

Sometimes the user is the one actually cleaning the data. In this case, (s)he can use \emph{P} and \emph{M} to quantify the value of her/his time. 
In other cases, cleaning is performed by other domain experts, who generally need to be paid. In this case, \emph{P} and \emph{M} are statements about budget priorities.



As a result, \dcv must make the following decisions.  First, it must make an assessment of the cleanliness of the result of any given join path.  We treat this issue in Section~\ref{subsec:model}. The obvious conclusion is to choose the join path that produces the highest quality result.
Then, we need to choose where to place,  in the resulting query plan, data cleaning operations to be the most efficient.  This is the topic of Section~\ref{subsec:gain}.


\subsection{The Cleanliness Model}
\label{subsec:model}

In \dcv, we collect information about the errors in each data set.\mourad{Do these assumptions makes sense!? I doubt anyone at Merck will volunteer to do it or even knows how to do it!} First, we assume that each data set owner gives us accuracy metrics for each column, namely an estimation for the percentage of the column which is erroneous. Second, the data owner also specifies a ``disorder metric" that indicates the distance between an incorrect value and its ground truth. For numeric values, we use the average and standard deviation as the disorder metric. For example, if salary errors average 5\% with standard deviation 2\%, then $\frac{2}{3}$ of the errors are less than 7\%. \footnote{\sibo{Where the 2/3 comes from? The distribution is not given in the context}}. For text values, we use the ``lexical distance'' (such as distance based on WordNet~\cite{WordNet,DBLP:journals/cacm/Miller95}) as the disorder metric. For example, if an address field routinely confuses ``\textit{road}'' and ``\textit{street}'', but very rarely gets the name of the street wrong, then the lexical distance is very small. \footnote{Previous comment: \srm{We should explicitly define different metrics for lexical and numeric.  Mean +/- std dev doesn’t make sense for lexical, I think.}}


To answer a query, \dcv generates a sub-graph for the query from the available join graph (which is the sub-graph of the linkage graph that only contains FK-PK edges) that covers all the joins in the query. For a given query, there may be multiple sub-graphs and we provide a multi-faceted interface for the user to pick one. One facet is the one with maximum cleanliness. Next we discuss how to achieve this according to the following cleanliness model.


We first define the cleanliness of an edge in the join graph. As in most cases we do not join on numeric values, we only consider lexical distance here.

Given two linked nodes \RX and \SY in the graph, when we align the values in \RX and \SY using the maximum matching defined in Section~\ref{subsec:eind}, we can tolerate some errors. Suppose that the maximum lexical distances in the two nodes are \Dis(\RX) and \Dis(\SY), respectively\footnote{Previous comment: \srm{is this an average over all values?}}. The distance between two values in \RX and \SY that represent the same object is should\footnote{Previous comment:\srm{Not really...}} be within $\Dis(\RX) + \Dis(\SY)$. Thus we limit the weight (i.e., the text similarity) of each alignment in the maximal matching to be no smaller than $2-(\Dis(\RX)+\Dis(\SY))$\footnote{Previous comment: \srm{This could be < 0. This is for lexical similarity;  what about for numeric quantities.}}\footnote{\dong{To guarantee the correctness here later}}. Then the fraction of the values in the foreign key that appear in the maximal matching is the cleanliness of the edge. 


We define the cleanliness of a node \RX simply as its accuracy \Acc(\RX), which means the percentage of values in \RX that are not erroneous.\footnote{Previous comment: \srm{No, DIS(R[X]) is the average dissimilaritity of the edge;  it doesn’t say anything about whetehr a given value is erroneous.}\dong{I mistakenly wrote DIS as ACC.}}

Given a join path, its cleanliness is obtained by multiplying the cleanliness values of all the edges, and the nodes involved in predicates which is necessary since errors can propagate along this path and is dominated by the dirtiest one.



\subsection{Query Plan with Data Cleaning Operation}
\label{subsec:gain}

Obviously expensive cleaning should be performed on as few records as possible. Hence, we would like to insert cleaning operations as late in the query plan as possible. 
Unfortunately, if we run a query with the following predicate:

\vspace{.5em}
\dots \textsf{where} $name = $ ``\code{New York}''
\vspace{.5em}

\noindent Then the misspelled city name,  \ie \code{New Yark}, will not be found, and accuracy will suffer. One solution is to clean the entire source data sets to avoid such errors, an expensive proposition indeed. If the penalty is small, then we can insert cleaning ``late'' in the query plan\srm{Why not use a relaxed similarity threshold and then clean those nodes.}. On the other hand, if the penalty is large, then we must insert cleaning earlier, even though at much higher cost. We formalize our idea as follows.


For each edge in the join path, if we put the cleaning operation ahead of the join operation, i.e., we only clean the tuple pairs in our maximal matching, we spend cleaning cost linear to the number of foreign keys and can only expect to achieve the same accuracy  as the cleanliness of the edge. In contrast, if we first clean all the possible tuple pairs of the foreign key and primary key and then generate the real join results, we can achieve 100\% accuracy with a cleaning cost quadratic to the number of foreign keys. Similarly, if we put the cleaning operation behind the where clause, we can only expect to achieve the same accuracy of the column with the cost proportional to the number of tuples satisfying the where clause. Otherwise, we can achieve 100\% accuracy with the cost proportional to the number tuples in the whole column.


In this setting, we can develop a dynamic programming algorithm to obtain a query plan with cleaning operations that achieve the maximum accuracy gain with limited cleaning cost budget or achieve the desired accuracy with the smallest cleaning cost budget\srm{What is algorithm?  Really dynamic programming?}\dong{Yes, do I need to describe more specifically?}. In the algorithm, there are three kinds of states for each edge and each node in the join graph: 
no cleaning, cleaning before query operation, and cleaning after query operation, each with different cleaning costs.
%do not apply cleaning operation on it, apply cleaning operation before the query operation, and apply cleaning operation after the query operation, each with different cleaning costs.


