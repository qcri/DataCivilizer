% !TEX root = ../main.tex
\section{Linkage Graph Computation}
\label{sec:stitching}
%\sibo{The term ``linkage graph computation'' is not consistent with the one``join path computation'' in abstract and introduction.} \mourad{fixed}
In this section, we
discuss how to build the linkage graph, as shown in Figure~\ref{fig:arch}, for the discovered data sets.
\nan{We need to state clearly here what is the input and output of this Section.}

\subsection{Graph Builder}
\label{subsec:graphbuild}

The linkage graph is a multi-graph\mourad{Raul and Dong need to agree on the structure of the graph, especially if nodes are tables or columns.} where each node represents a table and each
edge represents a relationship between the two tables or two columns in the two tables. 
Examples of relationships are 
\emph{column similarity}, 
\emph{schema similarity}, 
\emph{structure similarity}, % based on SimRank~\cite{DBLP:conf/kdd/JehW02}\mourad{No need to mention how since we don't for the others.}, 
\emph{inclusion dependency}, 
\emph{foreign-key and primary-key} , 
\emph{table subsuming}, and 
\emph{table equivalence}\mourad{What does it mean?}. 
The node label contains table meta-data, such as table name and schema information. 
The edge label includes its endpoints information (column names or table names) and
relationship information, \eg type and score if any. 
Computing the different relationships requires different time complexities.
We thus divide them into, {\textit light relationships}, which can be computed in sub-quadratic time,
and {\textit heavy relationships}, which needs at least quadratic time. 
The light relationships contain column similarity and schema similarity. 
The heavy relationships include the primary key-foreign key (PK-FK), inclusion dependency, and the structure similarity. 
%Next we discuss how to build the light relationships.


The input to the graph builder is a set of profiles that are attached to nodes of the graph. 
The graph builder then computes the relationships  among pairs of nodes. 
In particular, the graph builder first finds column similarity and schema similarity relationships. 
Each edge is assigned a weight that represents the strength of the relationship: the particular meaning depends on the edge semantics.  
Also, edges with a similarity less than a user-defined threshold are discarded to avoid having a graph with too many edges that are not significant. 
A straightforward computation of such relationships requires a $O(n^2)$ computation, which will not scale.  
Hence, we rely on locality sensitive hashing (LSH)~\cite{DBLP:conf/compgeom/DatarIIM04}, which runs in sub-quadratic time. This approach avoids $n^2$ operations and appears to perform well in practice.


It is usually expensive to compute the heavy relationships. 
To address this issue, the graph builder adopts  the following general approach: 
(i)~the computation of heavy relationships is performed in the background, 
(ii)~we need to come up with efficient algorithms to find these relationships, and
9iii)~the data profiles and light relationships should be used as much as possible to prune the search space. 
Note that the data discovery can use the heavy relationships, whenever available, to identify interesting data. 
After the users identify their interesting data, the graph builder can construct the heavy relationships among the interesting data online.


%For efficiently computing the heavy relationships, 
We take the foreign-key and primary-key relationship as an example to show how we efficiently compute a heavy relationship. 
The graph builder utilizes an implementation of inclusion dependency to find candidate PK-FK relationships and then refines the candidates using existing machine learning methods. 
However, as it is well known, data in the wild is quite dirty, which makes discovering inclusion dependencies difficult. Specifically, foreign keys may not match a primary key, because of errors in either the PK or the FK. To tolerate errors, we extend traditional inclusion dependency discovery by both key coverage and text similarity and propose an \emph{\eind}. We describe the error-robust inclusion dependency in Section~\ref{subsec:eind} and machine learning techniques to refine the candidate PK/FK relationships in Section~\ref{subsec:refine}.


\subsection{Error-Robust Inclusion Dependency}\label{subsec:eind}

Foreign key/primary key relationships are usually identified by inclusion dependency techniques. 
To address dirty data, we propose an error-robust inclusion dependency scheme.
%, which enhances traditional inclusion dependency techniques.
Consider two columns (or compound columns) from two tables \R and \S denoted by \RX and \SY. If there is a foreign key constraint on \RX with reference \SY, all the values in \RX must appear in \SY, which yields an inclusion dependency from \RX to \SY. However, in the real world, values in foreign key fields may not exist in the primary key fields due to errors.  In this case an inclusion dependency from foreign key to primary key does no hold.

To address the above issue, for each distinct value in \RX, we calculate the text similarity to values in \SY and use the maximum value as the strength of a value matching. We then compute the total strength of the maximum matching value divided by the number of values in \RX. This is the overall strength of the inclusion dependency.  
If this number exceeds a predefined threshold $\delta$, we specify an inclusion dependency from \RX to \SY in our linkage graph.
%\nan{This is very weak. We can also tolerate errors by saying if $> x\%$ \RX appears in \SY, then there is a related inclusion dependency.}

When dealing with compound columns, we can utilize different text similarity functions on different fields. 
Also, we must compose the individual column scores to achieve an overall strength.


\subsection{Refine Candidate FK-PK Relationships}\label{subsec:refine}


The error-robust inclusion dependency algorithm returns a collection of candidate PK-FK relationships.
We apply the algorithms in~\cite{DBLP:conf/webdb/RostinABNL09} to refine our candidate selection. The authors proposed 10~different features to distinguish foreign key constraints from spurious inclusion dependencies. Consider two columns \RX and \SY with an inclusion dependency from the first one to the second one. In~\cite{DBLP:conf/webdb/RostinABNL09}, the specified features include 
\emph{coverage}, which is the ratio of distinct values in \RX that are contained in \SY, 
\emph{column name similarity}, which is the similarity between the attribute names of X and Y, 
and \emph{out-of-domain range}, which is the percentage of values in \SY not within $[\min(\RX), \max(\RX)]$ where $\min(\RX)$ and $\max(\RX)$ are respectively the minimum and maximum values in \RX.
Using these features, we have implemented the four machine learning classification algorithms in~\cite{DBLP:conf/webdb/RostinABNL09}. These allow us to distinguish spurious inclusion dependencies from real ones  with high accuracy.

The result is a collection of PK-FK relationships that can be used to link the tables from the discovery module. However, there may exist multiple subgraphs in the linkage graph that can connect all the interesting tables, which we call \emph{join paths}. In the next section we discuss the choice of which one(s) to use to materialize a view for the user.

%\nan{So far, this section only talks about FK-PK relationships. In my opinion, these should be merged with graph builder.}

%\nan{Data Stitcher should talk about how to join the discovered tables, i.e., materialize a view, which should not be left to the next section.}
