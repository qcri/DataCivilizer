% !TEX root = ../main.tex
\section{Linkage Graph Computation}
\label{sec:stitching}
%\sibo{The term ``linkage graph computation'' is not consistent with the one``join path computation'' in abstract and introduction.} \mourad{fixed}
In this section, we discuss how to build the linkage graph, as shown in
Figure~\ref{fig:arch}, for the discovered data sets. The input to the graph
builder is a set of profiles and the output is a linkage graph.
%\nan{We need to state
%clearly here what is the input and output of this Section.}

\subsection{Graph Builder}
\label{subsec:graphbuild}

The linkage graph is a hyper-graph where each simple node represents a column, each hyper-node consists of multiple simple nodes and can represents a table or a compound key, and each simple edge represents a relationship between two nodes in the graph. Note the hyper-graph can express relationships involving more than two columns. This is useful to capture relationships amongst tables, \eg \pkfk relationship between compound keys.

Examples of relationships are 
\emph{column similarity}, 
\emph{schema similarity}, 
\emph{structure similarity}, % based on SimRank~\cite{DBLP:conf/kdd/JehW02}\mourad{No need to mention how since we don't for the others.}, 
\emph{inclusion dependency}, 
\emph{\pkfk relationship}, and
\emph{table subsumption}. 
The node label contains table metadata, such as table name, cardinality and
other information computed by the profiler. The edge label includes metadata
about the relationship it represents, \eg type and score, if any. Computing
the different relationships requires different time complexities. We 
categorize them into, \textit{light relationships}, which can be computed in
sub-quadratic time, and \textit{heavy relationships}, which needs at least
quadratic time. The light relationships contain column similarity and schema
similarity. The heavy relationships include the \pkfk (PK-FK), inclusion dependency, and structure similarity. 

The input to the graph builder is a set of profiles that are attached to nodes
of the graph.  The graph builder then computes the relationships amongst pairs of
nodes. In particular, the graph builder first finds column similarity and
schema similarity relationships.  Each edge is assigned a weight that represents
the strength of the relationship: the particular meaning depends on the edge
semantics.  Also, edges with a similarity less than a user-defined threshold are
discarded to avoid having a graph with too many edges that are not significant.
A straightforward computation of such relationships requires a $O(n^2)$
computation, which will not scale.  However, for light-relationships, which depend on similarity metric
such as edit distance and Jaccard similarlity, we can employ locality sensitive hashing
(LSH)~\cite{DBLP:conf/compgeom/DatarIIM04} to yield sub-quadratic runtime; we have found LSH to run quite well in practice, even for data sets up to several Terabytes.

It is usually expensive to compute the heavy relationships. To address this
issue, the graph builder adopts the following general approach: (i)~the
computation of heavy relationships is performed in the background, (ii)~data
profiles and light relationships are used as much as possible to prune the
search space for the heavy relationships, and (iii)~after the user chooses the data of
interest, if the heavy relationships are not ready amongst this data, the graph builder can construct the heavy relationships online.

%For efficiently computing the heavy relationships, 
We use the foreign-key (FK) and primary-key (PK) relationships as an example of how
we efficiently compute a heavy relationship and deal with the complexities that arise in real data where errors and noise make finding
such relationships tricky. The graph builder utilizes an
implementation of inclusion dependency (i.e., column A {\it includes all the values of} column B) to find candidate PK-FK relationships, since a PK should include all values in an FK column.  We then 
 refine the candidates using machine learning methods. However,
as it is well known, data in the wild is quite dirty, which makes discovering
inclusion dependencies difficult. Specifically, foreign keys may not match a
primary key, because of errors in either the PK or the FK. To tolerate errors,
we extend traditional inclusion dependency discovery by both key coverage and
text similarity and propose an \emph{\eind} approach. 
We describe this approach in Section~\ref{subsec:eind} and the machine learning
techniques to refine the candidate PK/FK relationships in Section~\ref{subsec:refine}.

\subsection{Error-Robust Inclusion Dependency}\label{subsec:eind}

As noted above, \Pkfk relationships are usually identified using inclusion
dependency techniques. To overcome the presence of dirty data, we propose an
error-robust inclusion dependency scheme.
%, which enhances traditional inclusion dependency techniques.
Consider two columns (or compound columns) from two tables \R and \S denoted by
\RX and \SY. If there is a foreign key constraint on \RX with reference \SY, all
the values in \RX must appear in \SY, which yields an inclusion dependency from
\RX to \SY. However, in the real world, values in foreign key fields may not
exist in the primary key fields due to errors.  In this case an inclusion
dependency from foreign key to primary key does no hold.

To address this, for each distinct value in \RX, we calculate the
text similarity to values in \SY and use the maximum value as the strength of a
value matching. We then compute the total strength of the maximum matching value
divided by the number of values in \RX. This is the overall strength of the
inclusion dependency.  If this number exceeds a predefined threshold $\delta$,
we add an inclusion dependency from \RX to \SY in our linkage graph.
%\nan{This is very weak. We can also tolerate errors by saying if $> x\%$ \RX
%appears in \SY, then there is a related inclusion dependency.}

When dealing with compound columns, we can utilize different text similarity functions on different fields. 
Also, we must compose the individual column scores to achieve an overall strength.


\subsection{Candidate PK-FK Relationships Refinement}
\label{subsec:refine}


The error-robust inclusion dependency algorithm returns a collection of candidate PK-FK relationships.
We use the algorithms in the work by Rostin et al.~\cite{DBLP:conf/webdb/RostinABNL09} 
%\nan{Ahmed's comment: Which is this system? Can it be named?}\mourad{It doesn't have a name!} 
to refine our candidate selection. The authors proposed 10~different features to distinguish foreign key constraints from spurious inclusion dependencies. Consider two columns \RX and \SY with an inclusion dependency from the first one to the second one. 
%In~\cite{DBLP:conf/webdb/RostinABNL09}, 
The specified features include 
\emph{coverage}, the ratio of distinct values in \RX that are contained in \SY, 
\emph{column name similarity}, the similarity between the attribute names of X and Y, 
and \emph{out-of-domain range}, the percentage of values in \SY not within $[\min(\RX), \max(\RX)]$ where $\min(\RX)$ and $\max(\RX)$ are respectively the minimum and maximum values in \RX.

Using the above features, we implemented the four machine learning classification algorithms from Rostin et al.~\cite{DBLP:conf/webdb/RostinABNL09}. These allow us to distinguish spurious inclusion dependencies from real ones  with high accuracy.
The result is a collection of PK-FK relationships that can be used to link the tables of user interest from the discovery module. However, there may exist multiple subgraphs in the linkage graph that can connect all the interesting tables, which we call \emph{join paths}. In the next section we discuss the choice of which one(s) to use to materialize a view for the user.

%\nan{So far, this section only talks about FK-PK relationships. In my opinion, these should be merged with graph builder.}

%\nan{Data Stitcher should talk about how to join the discovered tables, i.e., materialize a view, which should not be left to the next section.}
