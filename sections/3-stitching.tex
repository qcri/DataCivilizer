% !TEX root = ../main.tex
\section{Linkage Graph Computation}
\label{sec:stitching}
\sibo{The term ``linkage graph computation'' is not consistent with the one
``join path computation'' in abstract and introduction.} In this section, we
discuss how to build the linkage graph, as shown in Figure~\ref{fig:arch}, for the discovered data sets.



\subsection{Graph Builder}\label{subsec:graphbuild}



The linkage graph is a multi-graph where each node represents a table and each
edge represents a relationship between the two tables or two columns in the two
tables. Examples of relationships are \emph{column similarity}, \emph{schema
similarity}, \emph{structure similarity} based on SimRank~\cite{DBLP:conf/kdd/JehW02}, \emph{foreign-key and primary-key (PK-FK)} , \emph{inclusion dependency}, \emph{table subsuming}, and \emph{equivalence}. The node label contains table meta-data, such as table name and schema information. The edge label includes its endpoints information (such as column names) and relationship information (such as the type and the score). We divide the relationships to two kinds, the light relationship which can be found in sub-quadratic time and the heavy relationship which needs at least quadratic time. The light relationships contain column similarity and schema similarity. The heavy relationships include the PK-FK, inclusion dependency, and the structure similarity. Next we discuss how to build the light relationships.


The input to the graph builder is a set of profiles that are attached to nodes of the graph. The graph builder then computes the arcs among pairs of nodes. In particular, the graph builder first finds column similarity and schema similarity relationships. Each edge is scored with a weight that represents the strength of the relationship: the particular meaning depends on the edge semantics.  Also, edges with a similarity less than a user-defined threshold are discarded, to avoid having a graph with too many edges that are not significant. Finding these relationships in the graph is an $O(n^2)$ computation, which will not scale.  Hence, we rely on locality sensitive hashing (LSH)~\cite{DBLP:conf/compgeom/DatarIIM04}, which runs in sub-quadratic time. This approach avoids $n^2$ operations and appears to perform well in practice.



For the heavy relationships we take the foreign-key and primary-key relationship as an example. The graph builder utilizes an implementation of inclusion dependency to find candidate PK-FK relationships and then refines the candidates using existing machine learning methods. However, as is well known, data in the wild is quite dirty, which makes discovering inclusion dependencies difficult. Specifically, foreign keys may not match a primary key, because of errors in either the PK or the FK. To tolerate errors, we extend traditional inclusion dependency by both key coverage and text similarity and propose an \emph{\eind} which will be described presently.


It is expensive to compute the heavy relationships. To address this issue, in the graph builder, 1) the heavy relationships are keeping building in background, 2) efficient algorithms are proposed to find the heavy relationships, 3) and the data profiles and light relationships are used to prune the search space. Note that the data discovery can use the heavy relationships to identify interesting data when they are available. After the user identified their interesting data, the graph builder can construct the heavy relationships among the interesting data on-line.


Next we propose the error-robust inclusion dependency in Section~\ref{subsec:eind} and consider machine learning techniques to refine the candidate PK/FK relationships in Section~\ref{subsec:refine}.


\subsection{Error-Robust Inclusion Dependency}\label{subsec:eind}

Foreign key/primary key relationships are usually identified by inclusion dependency techniques. To address dirty data, we have designed an error-robust inclusion dependency scheme, which enhances traditional inclusion dependency techniques.

Consider two columns (or compound columns) from two tables \R and \S denoted by \RX and \SY. If there is a foreign key constraint on \RX with reference \SY, all the values in \RX must appear in \SY, which yields an inclusion dependency from \RX to \SY. However, in the real world, values in foreign key fields may not exist in the primary key fields due to errors.  In this case an inclusion dependency from foreign key to primary key does no hold.

To address this issue, for each distinct value in \RX, we calculate the text similarity to values in \SY and use the maximum value as the strength of a value matching. We then compute the total strength of the maximum matching value divided by the number of values in \RX. This is the overall strength of the inclusion dependency.  If this number exceeds a predefined threshold $\delta$, we specify an inclusion dependency from \RX to \SY in our linkage graph.
%\nan{This is very weak. We can also tolerate errors by saying if $> x\%$ \RX appears in \SY, then there is a related inclusion dependency.}

When dealing with compound columns, we can utilize different text similarity functions on different fields. Also, we must multiply the individual column scores to achieve an overall strength.


\subsection{Refine Candidate FK-PK Relationships}\label{subsec:refine}


The error-robust inclusion dependency algorithm gives us a collection of candidate PK-FK relationships.
We apply the algorithms in~\cite{DBLP:conf/webdb/RostinABNL09} to refine our candidate selection. The authors proposed 10 different features to distinguish foreign key constraints from spurious inclusion dependencies. Consider two columns \RX and \SY with an inclusion dependency from the first one to the second one. In~\cite{DBLP:conf/webdb/RostinABNL09}, the specified features include \emph{coverage}, which is the ratio of distinct values in \RX that are contained in \SY, \emph{column name similarity}, which is the similarity between the attribute names of X and Y, and the \emph{out-of-domain range}, which is the percentage of values in \SY not within $[\min(\RX), \max(\RX)]$ where $\min(\RX)$ and $\max(\RX)$ are respectively the minimum and maximum values in \RX.
Using these features, we have implemented the four machine learning classification algorithms in~\cite{DBLP:conf/webdb/RostinABNL09}. These allow us with high accuracy to distinguish spurious inclusion dependencies from real ones.

The result of data stitching is a collection of PK-FK relationships that can be used to link the tables from the discovery module. However, there may exist multiple subgraphs in the linkage graph that can connect all the interesting tables, which we call \emph{join paths}. In the next section we discuss the choice of which one(s) to use to materialize a view for the user.

%\nan{So far, this section only talks about FK-PK relationships. In my opinion, these should be merged with graph builder.}

%\nan{Data Stitcher should talk about how to join the discovered tables, i.e., materialize a view, which should not be left to the next section.}
