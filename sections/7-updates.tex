% !TEX root = ../main.tex
\section{Updates}
\label{sec:updates}

Real-world data is rarely static, which is exactly the scenario that \dcv faces at Merck and the M.I.T. Data Warehouse.  We consider three types of updates.

\stitle{(1) Insertions/deletions on source tables.} This happens when there is a change to a table, e.g., an insertions of new procurement record in the M.I.T. Data Warehouse. This may also happen when some data sources get cleaned (see Section~\ref{sec:curating}).



\stitle{(2) Replacement of source tables.} Large companies typically rely on both internal and external information to build their knowledge base. For instance, Merck collects published standard medical names from the World Health Organization (WHO) to help construct their own ontology. These information sources are updated periodically by WHO.  Sometimes, even the format will be changed, e.g., from a JSON file to a CSV file.



\stitle{(3) Updating MVs.} MVs might be created which depend on other MVs.  Since cleaning effort can happen at any place in a query plan, MVs may be updated.



In response to the above three types of updates, \dcv uses three corresponding strategies.



\stitle{(i) MV maintenance.} \dcv will leverage mature techniques for maintaining materialized views (see~\cite{DBLP:journals/debu/GuptaM95} for a survey).  In this way, \dcv incrementally propagates updates through the data curation pipeline to update downstream MVs.




\stitle{(ii) Provenance management.}  In case (2) above, the human effort to update and clean MVs may be daunting. In these cases, MVs should be discarded rather than updated. Naturally, there is need for a workflow component that supports data versioning and branching operations \dcv will leverage Decibel~\cite{DBLP:journals/pvldb/MaddoxGEMPD16}, a system developed at MIT for this purpose.




\stitle{(iii) Descriptive and prescriptive data cleaning.}When a scientist updates an MV (case 3) above), this will trigger update propagation to descendent data sets, as well as back upstream to data sources. To perform this propagation, we plan to leverage the techniques in~\cite{DBLP:conf/sigmod/ChalamallaIOP14}, a system developed by QCRI and Waterloo.


As data appears incrementally, we need an incremental inclusion dependency algorithm. Extending our algorithms in Section~\ref{subsec:eind} is a project we are currently working on.

